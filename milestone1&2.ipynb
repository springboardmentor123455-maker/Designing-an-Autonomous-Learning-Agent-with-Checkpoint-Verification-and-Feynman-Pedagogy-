{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_wOsWzDCPX6",
        "outputId": "f7a3d446-d803-4900-d74a-1928b6ec68d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-1.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tavily-python\n",
            "  Downloading tavily_python-0.7.17-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.5)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting reportlab\n",
            "  Downloading reportlab-4.4.6-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.12/dist-packages (0.4.59)\n",
            "Collecting groq<1.0.0,>=0.30.0 (from langchain-groq)\n",
            "  Downloading groq-0.37.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (1.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from tavily-python) (2.32.4)\n",
            "Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tavily-python) (0.12.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from tavily-python) (0.28.1)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.12.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.4)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith) (3.11.5)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langsmith) (25.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.12.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (1.33)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (9.1.2)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (2.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.5.1->tavily-python) (2025.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Downloading langchain_groq-1.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading tavily_python-0.7.17-py3-none-any.whl (18 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading reportlab-4.4.6-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.37.1-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab, PyPDF2, tavily-python, groq, langchain-groq\n",
            "Successfully installed PyPDF2-3.0.1 groq-0.37.1 langchain-groq-1.1.1 reportlab-4.4.6 tavily-python-0.7.17\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-groq tavily-python langgraph PyPDF2 reportlab sentence-transformers langsmith\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"GROQ_API_KEY\"\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"TAVILY_API_KEY\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"LANGSMITH_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os, re, json, datetime, html, time, hashlib\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, TypedDict\n",
        "from pathlib import Path\n",
        "import importlib\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from IPython.display import Markdown, display\n",
        "os.environ['EMBEDDING_DEBUG'] = '0'\n",
        "\n",
        "# Global buffer for ALL raw LLM logs\n",
        "\n",
        "GLOBAL_RAW_LOG: List[str] = []\n",
        "\n",
        "#  LangSmith tracing\n",
        "\n",
        "try:\n",
        "    from langsmith import traceable\n",
        "except ImportError:\n",
        "    # No-op decorator if langsmith not installed\n",
        "    def traceable(*targs, **tkwargs):\n",
        "        def decorator(fn):\n",
        "            return fn\n",
        "        return decorator\n",
        "\n",
        "# API Keys (Groq + Tavily)\n",
        "\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"\").strip()\n",
        "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\", \"\").strip()\n",
        "\n",
        "if not GROQ_API_KEY or not TAVILY_API_KEY:\n",
        "    raise SystemExit(\n",
        "        \"ERROR: GROQ_API_KEY and TAVILY_API_KEY must be set in environment.\\n\"\n",
        "        \"In Colab, run:\\n\"\n",
        "        \"import os\\n\"\n",
        "        \"os.environ['GROQ_API_KEY'] = 'your_groq_key'\\n\"\n",
        "        \"os.environ['TAVILY_API_KEY'] = 'your_tavily_key'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# External clients & PDF lib (reading)\n",
        "\n",
        "try:\n",
        "    from tavily import TavilyClient\n",
        "except Exception:\n",
        "    raise RuntimeError(\"Missing tavily library. Install with: pip install tavily-python\")\n",
        "\n",
        "# PDF lib for reading user PDFs\n",
        "_pdf_lib = None\n",
        "if importlib.util.find_spec(\"PyPDF2\"):\n",
        "    import PyPDF2 as _pdf_lib\n",
        "else:\n",
        "    _pdf_lib = None  # PDF ingestion will error if not installed\n",
        "\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "\n",
        "\n",
        "# PDF writer for saving raw LLM outputs\n",
        "\n",
        "try:\n",
        "    from reportlab.pdfgen import canvas\n",
        "    from reportlab.lib.pagesizes import letter\n",
        "    _pdf_writer_available = True\n",
        "except ImportError:\n",
        "    _pdf_writer_available = False\n",
        "\n",
        "\n",
        "def save_raw_to_pdf(text: str, filename: str):\n",
        "    if not _pdf_writer_available:\n",
        "        print(\"[PDF] reportlab not installed; skipping PDF save.\")\n",
        "        return\n",
        "    try:\n",
        "        c = canvas.Canvas(filename, pagesize=letter)\n",
        "        width, height = letter\n",
        "        x_margin = 40\n",
        "        y = height - 50\n",
        "        max_width_chars = 110\n",
        "        for line in text.split(\"\\n\"):\n",
        "            while len(line) > max_width_chars:\n",
        "                chunk = line[:max_width_chars]\n",
        "                line = line[max_width_chars:]\n",
        "                if y < 40:\n",
        "                    c.showPage()\n",
        "                    y = height - 50\n",
        "                c.drawString(x_margin, y, chunk)\n",
        "                y -= 15\n",
        "            if y < 40:\n",
        "                c.showPage()\n",
        "                y = height - 50\n",
        "            c.drawString(x_margin, y, line)\n",
        "            y -= 15\n",
        "        c.save()\n",
        "        print(f\"[PDF Saved] {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[PDF ERROR] Failed to save PDF '{filename}': {e}\")\n",
        "\n",
        "\n",
        "def save_all_raw_to_one_pdf(filename: str = \"all_llm_raw_output.pdf\"):\n",
        "    if not GLOBAL_RAW_LOG:\n",
        "        print(\"[PDF] No raw LLM data to save.\")\n",
        "        return\n",
        "    full_text = \"\\n\".join(GLOBAL_RAW_LOG)\n",
        "    save_raw_to_pdf(full_text, filename)\n",
        "\n",
        "\n",
        "\n",
        "# Groq LLM via LangChain\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "GROQ_MODEL = os.getenv(\"GROQ_MODEL\", \"llama-3.1-8b-instant\")\n",
        "_LLM_CACHE_PATH = Path(\"/tmp/groq_inference_cache.json\")\n",
        "try:\n",
        "    _LLM_CACHE = json.load(open(_LLM_CACHE_PATH)) if _LLM_CACHE_PATH.exists() else {}\n",
        "except Exception:\n",
        "    _LLM_CACHE = {}\n",
        "\n",
        "\n",
        "def _save_llm_cache():\n",
        "    try:\n",
        "        json.dump(_LLM_CACHE, open(_LLM_CACHE_PATH, \"w\"), indent=2, ensure_ascii=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def _prompt_key(prompt: str) -> str:\n",
        "    return hashlib.sha256(prompt.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "\n",
        "groq_llm = ChatGroq(\n",
        "    model=GROQ_MODEL,\n",
        "    temperature=0.0,\n",
        "    max_tokens=800,\n",
        "    groq_api_key=GROQ_API_KEY,\n",
        ")\n",
        "\n",
        "\n",
        "@traceable(name=\"groq_llm_call\")\n",
        "def groq_chat(prompt: str, max_retries: int = 3, max_new_tokens: int = 800, timeout: int = 60) -> str:\n",
        "\n",
        "    key = _prompt_key(prompt)\n",
        "    if key in _LLM_CACHE:\n",
        "        txt = _LLM_CACHE[key]\n",
        "        GLOBAL_RAW_LOG.append(\n",
        "            f\"\\n\\n=== CACHED LLM CALL ===\\nTimestamp: {datetime.datetime.now(datetime.timezone.utc).isoformat()}\\n\"\n",
        "            f\"Prompt:\\n{prompt}\\n\\nResponse:\\n{txt}\\n\"\n",
        "        )\n",
        "        return txt\n",
        "    try:\n",
        "        resp = groq_llm.invoke(prompt)\n",
        "    except Exception as e:\n",
        "        print(f\"Groq LLM error: {e}\")\n",
        "        return \"\"\n",
        "    txt = \"\"\n",
        "    try:\n",
        "        if hasattr(resp, \"content\"):\n",
        "            if isinstance(resp.content, str):\n",
        "                txt = resp.content\n",
        "            elif isinstance(resp.content, list):\n",
        "                txt = \"\".join(str(part) for part in resp.content)\n",
        "            else:\n",
        "                txt = str(resp.content)\n",
        "        else:\n",
        "            txt = str(resp)\n",
        "    except Exception:\n",
        "        txt = str(resp)\n",
        "    txt = (txt or \"\").strip()\n",
        "    _LLM_CACHE[key] = txt\n",
        "    _save_llm_cache()\n",
        "    GLOBAL_RAW_LOG.append(\n",
        "        f\"\\n\\n=== RAW LLM CALL ===\\nTimestamp: {datetime.datetime.now(datetime.timezone.utc).isoformat()}\\n\"\n",
        "        f\"Prompt:\\n{prompt}\\n\\nResponse:\\n{txt}\\n\"\n",
        "    )\n",
        "    return txt\n",
        "\n",
        "\n",
        "\n",
        "# Checkpoint structure\n",
        "\n",
        "@dataclass\n",
        "class Checkpoint:\n",
        "    id: str\n",
        "    topic: str\n",
        "    objectives: List[str]\n",
        "    success_criteria: str\n",
        "\n",
        "\n",
        "CHECKPOINTS = [\n",
        "    Checkpoint(\n",
        "        id=\"cp1\",\n",
        "        topic=\"Basics of Neural Networks\",\n",
        "        objectives=[\n",
        "            \"Understand what an artificial neuron is\",\n",
        "            \"Understand input, hidden, and output layers\",\n",
        "            \"Understand the concept of forward propagation\",\n",
        "        ],\n",
        "        success_criteria=\"Learner can explain a simple feedforward neural network.\",\n",
        "    ),\n",
        "    Checkpoint(\n",
        "        id=\"cp2\",\n",
        "        topic=\"Gradient Descent\",\n",
        "        objectives=[\n",
        "            \"Understand loss minimization\",\n",
        "            \"Understand gradient as slope of loss\",\n",
        "            \"Understand iterative parameter updates\",\n",
        "        ],\n",
        "        success_criteria=\"Learner can describe how gradient descent updates parameters.\",\n",
        "    ),\n",
        "    Checkpoint(\n",
        "        id=\"cp3\",\n",
        "        topic=\"Activation Functions\",\n",
        "        objectives=[\n",
        "            \"Know common activations (sigmoid, tanh, relu)\",\n",
        "            \"Understand when/why to use each\",\n",
        "        ],\n",
        "        success_criteria=\"Learner can choose and justify an activation for a simple task.\",\n",
        "    ),\n",
        "    Checkpoint(\n",
        "        id=\"cp4\",\n",
        "        topic=\"Backpropagation\",\n",
        "        objectives=[\n",
        "            \"Understand chain rule for gradients\",\n",
        "            \"Understand how weight updates propagate backward\",\n",
        "        ],\n",
        "        success_criteria=\"Learner can explain backpropagation at high level.\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Agent State\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    cp_id: str\n",
        "    checkpoint: Optional[Checkpoint]\n",
        "    user_notes: str\n",
        "    user_pdfs: List[str]\n",
        "    gathered_context: str\n",
        "    context_sources: List[str]\n",
        "    relevance_score_model: Optional[int]\n",
        "    refetch_attempted: bool\n",
        "    score_meta: Optional[dict]\n",
        "\n",
        "    processed_chunks: List[str]\n",
        "    questions: List[str]\n",
        "    learner_answers: List[str]\n",
        "    score_percent: Optional[float]\n",
        "    pass_threshold_met: Optional[bool]\n",
        "    # added field:\n",
        "    temp_vector_store: Optional[dict]\n",
        "\n",
        "\n",
        "_score_re = re.compile(r\"\\b([1-5])\\b\")\n",
        "\n",
        "\n",
        "def parse_score_from_text(raw: str) -> int:\n",
        "    if not raw:\n",
        "        return 3\n",
        "    m = _score_re.search(raw)\n",
        "    return int(m.group(1)) if m else 3\n",
        "\n",
        "\n",
        "\n",
        "# PDF extraction helpers\n",
        "\n",
        "def extract_text_from_pdf(path: str) -> str:\n",
        "    if not _pdf_lib:\n",
        "        raise RuntimeError(\"PyPDF2 not installed. `pip install PyPDF2` to enable PDF ingestion.\")\n",
        "    text = []\n",
        "    with open(path, \"rb\") as f:\n",
        "        reader = _pdf_lib.PdfReader(f)\n",
        "        for p in reader.pages:\n",
        "            try:\n",
        "                page_text = p.extract_text() or \"\"\n",
        "            except Exception:\n",
        "                page_text = \"\"\n",
        "            if page_text:\n",
        "                text.append(page_text)\n",
        "    return \"\\n\".join(text)\n",
        "\n",
        "\n",
        "def gather_texts_from_pdfs(paths: List[str]) -> str:\n",
        "    out = \"\"\n",
        "    for p in paths:\n",
        "        try:\n",
        "            t = extract_text_from_pdf(p)\n",
        "            if t.strip():\n",
        "                out += f\"\\n--- PDF: {os.path.basename(p)} ---\\n{t}\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to read PDF {p}: {e}\")\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "# Summarizer (uses Groq)\n",
        "\n",
        "def summarize_text(text: str) -> str:\n",
        "    if not text.strip():\n",
        "        return text\n",
        "    prompt = f\"\"\"\n",
        "Summarize the following text into a focused explanation matching the learning objectives.\n",
        "Keep it concise, clean, and relevant.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{text[:5000]}\\\"\\\"\\\"\\n\n",
        "Summary:\n",
        "\"\"\"\n",
        "    summary = groq_chat(prompt)\n",
        "    return summary.strip() or text[:5000]\n",
        "\n",
        "\n",
        "\n",
        "# Tavily wrapper with rate limiting (< 10 searches/min)\n",
        "\n",
        "_search_timestamps: List[float] = []\n",
        "\n",
        "\n",
        "def _enforce_search_rate_limit():\n",
        "    global _search_timestamps\n",
        "    now = time.time()\n",
        "    _search_timestamps = [t for t in _search_timestamps if now - t < 60]\n",
        "    if len(_search_timestamps) >= 9:\n",
        "        oldest = min(_search_timestamps)\n",
        "        wait = 60 - (now - oldest)\n",
        "        if wait > 0:\n",
        "            print(f\"[Rate Limit] Tavily search rate reached. Waiting {wait:.1f}s to stay under 10/min...\")\n",
        "            time.sleep(wait)\n",
        "        now = time.time()\n",
        "        _search_timestamps = [t for t in _search_timestamps if now - t < 60]\n",
        "    _search_timestamps.append(time.time())\n",
        "\n",
        "\n",
        "@traceable(name=\"tavily_search\")\n",
        "def search_tavily(query: str, max_results: int = 5) -> List[dict]:\n",
        "    _enforce_search_rate_limit()\n",
        "    try:\n",
        "        res = tavily_client.search(query=query, max_results=max_results)\n",
        "        return res.get(\"results\", []) if isinstance(res, dict) else []\n",
        "    except Exception as e:\n",
        "        print(\"Tavily search failed:\", e)\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "# Evidence cleaning & user JSON helpers\n",
        "\n",
        "def clean_evidence(raw: str) -> str:\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    s = raw\n",
        "    s = re.sub(r\"```.*?```\", \"\", s, flags=re.DOTALL)\n",
        "    s = re.sub(r\"\\s*\\\\n\\s*\", \" \", s)\n",
        "    s = re.sub(r'^\\s*\\{.*?\"evidence\"\\s*:\\s*', \"\", s, flags=re.DOTALL)\n",
        "    s = s.replace('\"covered\":', \"\")\n",
        "    s = s.replace(\"{\", \"\").replace(\"}\", \"\")\n",
        "    s = s.replace('\"\"\"', \"\").replace(\"'''\", \"\")\n",
        "    s = s.strip()\n",
        "    s = re.sub(r'^\\s*[\"\\']?evidence[\"\\']?\\s*[:\\-]?\\s*', \"\", s, flags=re.I)\n",
        "    s = html.unescape(s).strip()\n",
        "    if len(s) > 300:\n",
        "        s = s[:300].rsplit(\" \", 1)[0] + \"...\"\n",
        "    return s\n",
        "\n",
        "\n",
        "def simplify_score_meta_for_user(score_meta: Optional[dict]):\n",
        "    if not score_meta:\n",
        "        return None\n",
        "    covered = score_meta.get(\"covered_count\", 0)\n",
        "    total = score_meta.get(\"total\", 1)\n",
        "    objectives = []\n",
        "    for d in score_meta.get(\"details\", []):\n",
        "        objectives.append(\n",
        "            {\n",
        "                \"objective\": d.get(\"objective\"),\n",
        "                \"covered\": True if str(d.get(\"covered\", \"no\")).lower() == \"yes\" else False,\n",
        "                \"evidence\": clean_evidence(d.get(\"evidence\", \"\")),\n",
        "            }\n",
        "        )\n",
        "    coverage_percent = int(round((covered / total) * 100))\n",
        "    summary = f\"{covered}/{total} objectives covered\"\n",
        "    explain = f\"{coverage_percent}% — {summary}\"\n",
        "    return {\n",
        "        \"coverage_percent\": coverage_percent,\n",
        "        \"summary\": summary,\n",
        "        \"explain\": explain,\n",
        "        \"objective_reports\": objectives,\n",
        "    }\n",
        "\n",
        "\n",
        "# Embeddings + Temporary in-memory vector store (Milestone 2)\n",
        "\n",
        "_emb_model = None\n",
        "_np = None\n",
        "_EMBEDDING_DEBUG = os.getenv(\"EMBEDDING_DEBUG\", \"0\") in (\"1\", \"true\", \"True\")\n",
        "\n",
        "# Chunking configuration\n",
        "CHUNK_SIZE = 1200\n",
        "CHUNK_OVERLAP = 250\n",
        "MIN_CHUNK_LENGTH = 300\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import numpy as np\n",
        "\n",
        "    try:\n",
        "        print(\"[Embedding] Loading SentenceTransformer 'all-MiniLM-L6-v2' ... (this may take a few seconds)\")\n",
        "        _emb_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "        _np = np\n",
        "        print(\"[Embedding] Model loaded:all-MiniLM-L6-v2\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Embedding] Failed to load SentenceTransformer model: {e}\")\n",
        "        _emb_model = None\n",
        "        _np = None\n",
        "except Exception:\n",
        "    _emb_model = None\n",
        "    try:\n",
        "        import numpy as np\n",
        "        _np = np\n",
        "    except Exception:\n",
        "        _np = None\n",
        "\n",
        "\n",
        "def is_context_relevant_semantically(\n",
        "    context: str,\n",
        "    cp,\n",
        "    threshold: float = 0.35\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Embedding-based semantic relevance check.\n",
        "    Returns False if context is unrelated to checkpoint topic/objectives.\n",
        "    \"\"\"\n",
        "    if not context.strip():\n",
        "        return False\n",
        "\n",
        "    # If embeddings unavailable, don't block pipeline\n",
        "    if _emb_model is None or _np is None:\n",
        "        return True\n",
        "\n",
        "    reference_text = cp.topic + \" \" + \" \".join(cp.objectives)\n",
        "\n",
        "    try:\n",
        "        ctx_vec = _emb_model.encode([context[:2000]], convert_to_numpy=True)\n",
        "        ref_vec = _emb_model.encode([reference_text], convert_to_numpy=True)\n",
        "\n",
        "        ctx_vec = ctx_vec / (_np.linalg.norm(ctx_vec, axis=1, keepdims=True) + 1e-12)\n",
        "        ref_vec = ref_vec / (_np.linalg.norm(ref_vec, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "        similarity = float(ctx_vec @ ref_vec.T)\n",
        "\n",
        "        if _EMBEDDING_DEBUG:\n",
        "            print(f\"[Semantic Relevance] similarity={similarity:.3f}\")\n",
        "\n",
        "        return similarity >= threshold\n",
        "\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_temp_vector_store(chunks: List[str]):\n",
        "    \"\"\"\n",
        "    Build temporary in-memory vector store for this session.\n",
        "    Returns dict: { 'chunks': [...], 'vectors': np.array or None, 'meta': {...} }\n",
        "    \"\"\"\n",
        "    if not chunks:\n",
        "        return {\"chunks\": [], \"vectors\": None, \"meta\": {\"embeddings_used\": False}}\n",
        "    if _emb_model and _np is not None:\n",
        "        try:\n",
        "            vecs = _emb_model.encode(chunks, convert_to_numpy=True, show_progress_bar=False)\n",
        "            # normalize vectors for cosine sim\n",
        "            norms = _np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-12\n",
        "            vecs = vecs / norms\n",
        "            store = {\"chunks\": chunks, \"vectors\": vecs, \"meta\": {\"embeddings_used\": True}}\n",
        "            if _EMBEDDING_DEBUG:\n",
        "                print(f\"[Embedding] Built vector store: vectors shape = {vecs.shape}\")\n",
        "            return store\n",
        "        except Exception as e:\n",
        "            print(f\"[Embedding] Exception while encoding chunks: {e}\")\n",
        "            pass\n",
        "    # fallback: no embeddings (vectors=None) — token-overlap will be used\n",
        "    if _EMBEDDING_DEBUG:\n",
        "        print(\"[Embedding] No embedding model available; using token-overlap fallback.\")\n",
        "    return {\"chunks\": chunks, \"vectors\": None, \"meta\": {\"embeddings_used\": False}}\n",
        "\n",
        "\n",
        "def embedding_debug_print(store, label: str = \"\"):\n",
        "    try:\n",
        "        emb_used = bool(store and store.get(\"vectors\") is not None)\n",
        "        if emb_used:\n",
        "            vecs = store[\"vectors\"]\n",
        "            shape = getattr(vecs, \"shape\", None)\n",
        "            print(f\"[Embedding Confirm] {label} embeddings used: True | vector shape: {shape}\")\n",
        "            # print a tiny sample: first vector first 6 values\n",
        "            sample = vecs[0][:6].tolist() if hasattr(vecs[0], \"tolist\") else list(vecs[0][:6])\n",
        "            print(f\"[Embedding Confirm] sample vec[0][:6] ~ {sample}\")\n",
        "        else:\n",
        "            print(f\"[Embedding Confirm] {label} embeddings used: False\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Embedding Confirm] Error during debug print: {e}\")\n",
        "\n",
        "\n",
        "def retrieve_top_k(store, query: str, k: int = 3) -> List[str]:\n",
        "    chunks = store.get(\"chunks\", []) or []\n",
        "    vectors = store.get(\"vectors\", None)\n",
        "\n",
        "    if not chunks:\n",
        "        return []\n",
        "\n",
        "    if vectors is not None and _emb_model is not None and _np is not None:\n",
        "        try:\n",
        "            qv = _emb_model.encode([query], convert_to_numpy=True, show_progress_bar=False)\n",
        "            qv = qv / (_np.linalg.norm(qv, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "            sims = vectors @ qv.T\n",
        "            sims = sims.reshape(-1)   # ✅ CRITICAL FIX\n",
        "\n",
        "            idx_sorted = sims.argsort()[::-1][:k]\n",
        "            top_chunks = [chunks[i] for i in idx_sorted if i < len(chunks)]\n",
        "\n",
        "            if _EMBEDDING_DEBUG:\n",
        "                top_info = [(int(i), float(sims[i])) for i in idx_sorted]\n",
        "                print(f\"[Retrieve Debug] top-k indices+sim: {top_info}\")\n",
        "\n",
        "            return top_chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[Retrieve] Embedding retrieval failed: {e}\")\n",
        "\n",
        "    # ---- fallback: token overlap ----\n",
        "    q_words = set(re.findall(r\"\\w+\", query.lower()))\n",
        "    scored = []\n",
        "    for i, c in enumerate(chunks):\n",
        "        c_words = set(re.findall(r\"\\w+\", c.lower()))\n",
        "        scored.append((len(q_words & c_words), i))\n",
        "\n",
        "    scored.sort(reverse=True)\n",
        "    return [chunks[i] for s, i in scored[:k] if s > 0] or chunks[:k]\n",
        "\n",
        "    if top:\n",
        "        if _EMBEDDING_DEBUG:\n",
        "            print(f\"[Retrieve Debug] token-overlap scores (top): {scored[:k]}\")\n",
        "        return top\n",
        "    # last fallback: first k chunks\n",
        "    return chunks[:k]\n",
        "\n",
        "\n",
        "\n",
        "# LangGraph Nodes\n",
        "\n",
        "def get_checkpoint_by_id(cp_id: str) -> Checkpoint:\n",
        "    for cp in CHECKPOINTS:\n",
        "        if cp.id == cp_id:\n",
        "            return cp\n",
        "    raise ValueError(f\"Checkpoint {cp_id} not found\")\n",
        "\n",
        "\n",
        "def start_checkpoint(state: AgentState) -> AgentState:\n",
        "    cp = get_checkpoint_by_id(state[\"cp_id\"])\n",
        "    state = dict(state)\n",
        "    state[\"checkpoint\"] = cp\n",
        "    state[\"gathered_context\"] = \"\"\n",
        "    state[\"context_sources\"] = []\n",
        "    state[\"score_meta\"] = None\n",
        "    state[\"processed_chunks\"] = []\n",
        "    state[\"questions\"] = []\n",
        "    state[\"score_percent\"] = None\n",
        "    state[\"pass_threshold_met\"] = None\n",
        "    state[\"temp_vector_store\"] = None\n",
        "    return state\n",
        "\n",
        "\n",
        "@traceable(name=\"gather_context_node\")\n",
        "def gather_context(state: AgentState) -> AgentState:\n",
        "    cp = state[\"checkpoint\"]\n",
        "    notes = state[\"user_notes\"]\n",
        "    pdfs = state.get(\"user_pdfs\", [])\n",
        "    context = \"\"\n",
        "    sources = []\n",
        "    if notes.strip():\n",
        "        context += f\"User Notes:\\n{notes.strip()}\\n\"\n",
        "        sources.append(\"user_notes\")\n",
        "    if pdfs:\n",
        "        pdf_text = gather_texts_from_pdfs(pdfs)\n",
        "        if pdf_text.strip():\n",
        "            context += pdf_text\n",
        "            sources.append(\"pdf_upload\")\n",
        "    if not context.strip():\n",
        "        query = f\"{cp.topic} - \" + \"; \".join(cp.objectives)\n",
        "        results = search_tavily(query=query)\n",
        "        for item in results:\n",
        "            content = item.get(\"content\")\n",
        "            if content:\n",
        "                context += content + \"\\n\"\n",
        "        if context.strip():\n",
        "            sources.append(\"web_search\")\n",
        "            context = summarize_text(context)\n",
        "    if context.strip() and len(context) > 5000:\n",
        "        context = summarize_text(context)\n",
        "    state = dict(state)\n",
        "    state[\"gathered_context\"] = context\n",
        "    state[\"context_sources\"] = sources\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"[Gathered Context] Source(s): {', '.join(sources) if sources else 'None'}\")\n",
        "    print(\"-\"*80)\n",
        "    print(context.strip() if context.strip() else \"[No context gathered]\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "@traceable(name=\"validate_context_node\")\n",
        "def validate_context(state: AgentState) -> AgentState:\n",
        "    cp = state[\"checkpoint\"]\n",
        "    context = state[\"gathered_context\"]\n",
        "    refetch = state[\"refetch_attempted\"]\n",
        "\n",
        "    #  SEMANTIC FILTER\n",
        "    is_relevant = is_context_relevant_semantically(context, cp)\n",
        "\n",
        "    if not is_relevant and not refetch:\n",
        "        print(\"[Semantic Filter] Context is unrelated. Refetching from web...\")\n",
        "\n",
        "        query = f\"{cp.topic} for beginners; \" + \"; \".join(cp.objectives)\n",
        "        results = search_tavily(query=query)\n",
        "\n",
        "        new_context = \"\"\n",
        "        for item in results:\n",
        "            c = item.get(\"content\")\n",
        "            if c:\n",
        "                new_context += c + \"\\n\"\n",
        "\n",
        "        if new_context.strip():\n",
        "            new_context = summarize_text(new_context)\n",
        "\n",
        "        context = new_context\n",
        "        refetch = True\n",
        "\n",
        "\n",
        "    def score_ctx_objectives_only(ctx: str):\n",
        "        objectives = cp.objectives\n",
        "        covered = 0\n",
        "        details = []\n",
        "\n",
        "        for obj in objectives:\n",
        "            prompt = f\"\"\"\n",
        "You must respond with a single valid JSON object and NOTHING else.\n",
        "\n",
        "Keys:\n",
        "- \"covered\": either \"yes\" or \"no\"\n",
        "- \"evidence\": one short sentence (<=30 words) quoting or paraphrasing the CONTEXT\n",
        "\n",
        "OBJECTIVE:\n",
        "\\\"\\\"\\\"{obj}\\\"\\\"\\\"\\n\n",
        "CONTEXT:\n",
        "\\\"\\\"\\\"{ctx[:8000]}\\\"\\\"\\\"\\n\n",
        "Return only JSON like: {{ \"covered\": \"yes\", \"evidence\": \"...\" }}\n",
        "\"\"\"\n",
        "            raw = groq_chat(prompt).strip()\n",
        "\n",
        "            cov = \"no\"\n",
        "            evidence = \"\"\n",
        "\n",
        "            try:\n",
        "                import json as _json, re as _re\n",
        "                m = _re.search(r\"\\{.*\\}\", raw, _re.DOTALL)\n",
        "                if m:\n",
        "                    parsed = _json.loads(m.group(0))\n",
        "                    cov = str(parsed.get(\"covered\", \"no\")).lower()\n",
        "                    evidence = str(parsed.get(\"evidence\", \"\")).strip()\n",
        "                    if cov not in (\"yes\", \"no\"):\n",
        "                        cov = \"no\"\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            if cov == \"yes\":\n",
        "                covered += 1\n",
        "\n",
        "            details.append({\n",
        "                \"objective\": obj,\n",
        "                \"covered\": cov,\n",
        "                \"evidence\": evidence\n",
        "            })\n",
        "\n",
        "        total = len(objectives) or 1\n",
        "        base_score = round((covered / total) * 5)\n",
        "        score = min(5, max(1, base_score))\n",
        "\n",
        "        meta = {\n",
        "            \"covered_count\": covered,\n",
        "            \"total\": total,\n",
        "            \"details\": details\n",
        "        }\n",
        "        return score, meta\n",
        "\n",
        "    score, score_meta = score_ctx_objectives_only(context)\n",
        "\n",
        "    if score <= 2 and not refetch:\n",
        "        print(\"Low relevance detected. Refetching...\")\n",
        "        query = f\"{cp.topic} for beginners; \" + \"; \".join(cp.objectives)\n",
        "        results = search_tavily(query=query)\n",
        "\n",
        "        new_context = \"\"\n",
        "        for item in results:\n",
        "            c = item.get(\"content\")\n",
        "            if c:\n",
        "                new_context += c + \"\\n\"\n",
        "\n",
        "        if new_context.strip():\n",
        "            new_context = summarize_text(new_context)\n",
        "\n",
        "        score, score_meta = score_ctx_objectives_only(new_context)\n",
        "        context = new_context\n",
        "        refetch = True\n",
        "\n",
        "    state = dict(state)\n",
        "    state[\"gathered_context\"] = context\n",
        "    state[\"relevance_score_model\"] = score\n",
        "    state[\"refetch_attempted\"] = refetch\n",
        "    state[\"score_meta\"] = score_meta\n",
        "\n",
        "    if refetch and \"web_search\" not in state[\"context_sources\"]:\n",
        "        state[\"context_sources\"].append(\"web_search\")\n",
        "\n",
        "    print(f\"Score for {cp.id}: {score} ({score_meta['covered_count']}/{score_meta['total']} objectives covered)\")\n",
        "    for d in score_meta[\"details\"]:\n",
        "        print(f\" - Obj: {d['objective'][:60]}... => {d['covered']} | evidence: {clean_evidence(d['evidence'])[:140]}\")\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "\n",
        "# Milestone 2: Context processing (chunking) + build temp vector store\n",
        "\n",
        "@traceable(name=\"process_context_node\")\n",
        "def process_context(state: AgentState) -> AgentState:\n",
        "    context = (state.get(\"gathered_context\") or \"\").strip()\n",
        "    chunks: List[str] = []\n",
        "\n",
        "    if context:\n",
        "        start = 0\n",
        "        text_len = len(context)\n",
        "\n",
        "        while start < text_len:\n",
        "            end = start + CHUNK_SIZE\n",
        "            chunk = context[start:end].strip()\n",
        "\n",
        "            if len(chunk) >= MIN_CHUNK_LENGTH:\n",
        "                chunks.append(chunk)\n",
        "\n",
        "            # move forward with overlap\n",
        "            start = end - CHUNK_OVERLAP\n",
        "            if start < 0:\n",
        "                start = 0\n",
        "\n",
        "            # stop infinite loop\n",
        "            if start >= text_len:\n",
        "                break\n",
        "\n",
        "    # Fallback safety\n",
        "    if not chunks and context:\n",
        "        chunks = [context]\n",
        "\n",
        "    state = dict(state)\n",
        "    state[\"processed_chunks\"] = chunks\n",
        "\n",
        "    # Build temporary vector store (Milestone 2 requirement)\n",
        "    store = build_temp_vector_store(chunks)\n",
        "    state[\"temp_vector_store\"] = store\n",
        "\n",
        "    # Debug confirmation\n",
        "    if _EMBEDDING_DEBUG:\n",
        "        print(f\"[Chunking] Produced {len(chunks)} chunks \"\n",
        "              f\"(size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})\")\n",
        "        embedding_debug_print(store, label=f\"cp={state.get('cp_id')}\")\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Milestone 2: Question generation (uses retrieval of top-k chunks to focus generation)\n",
        "\n",
        "@traceable(name=\"generate_questions_node\")\n",
        "def generate_questions(state: AgentState) -> AgentState:\n",
        "    cp = state[\"checkpoint\"]\n",
        "    chunks = state.get(\"processed_chunks\") or []\n",
        "    store = state.get(\"temp_vector_store\") or {\"chunks\": chunks, \"vectors\": None, \"meta\": {\"embeddings_used\": False}}\n",
        "\n",
        "    # Use retrieval: combine top-k chunks for question generation\n",
        "    focus_for_generation = \"\"\n",
        "    if chunks:\n",
        "        query = cp.topic + \" \" + \" \".join(cp.objectives)\n",
        "        top_chunks = retrieve_top_k(store, query=query, k=3)\n",
        "        focus_for_generation = \"\\n\\n\".join(top_chunks)\n",
        "    else:\n",
        "        focus_for_generation = \"\\n\\n\".join(chunks)[:4000]\n",
        "\n",
        "    base_context = (focus_for_generation or \"\\n\\n\".join(chunks))[:4000]\n",
        "    prompt = f\"\"\"\n",
        "You are a helpful tutor.\n",
        "\n",
        "Based on the following CONTEXT and LEARNING OBJECTIVES,\n",
        "generate 3 to 5 short, focused questions that test conceptual understanding.\n",
        "Avoid yes/no questions. Make them open-ended but concise.\n",
        "\n",
        "Return ONLY a JSON object like:\n",
        "{{ \"questions\": [\"Q1 ...\", \"Q2 ...\", \"...\"] }}\n",
        "\n",
        "CONTEXT:\n",
        "\\\"\\\"\\\"{base_context}\\\"\\\"\\\"\\n\n",
        "OBJECTIVES:\n",
        "{json.dumps(cp.objectives, ensure_ascii=False)}\n",
        "\"\"\"\n",
        "    raw = groq_chat(prompt).strip()\n",
        "    questions: List[str] = []\n",
        "    try:\n",
        "        m = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "        if m:\n",
        "            data = json.loads(m.group(0))\n",
        "            qlist = data.get(\"questions\", [])\n",
        "            if isinstance(qlist, list):\n",
        "                questions = [q.strip() for q in qlist if isinstance(q, str) and q.strip()]\n",
        "    except Exception:\n",
        "        pass\n",
        "    if not questions:\n",
        "        for line in raw.splitlines():\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            line = re.sub(r\"^[\\-\\*\\d\\.\\)\\s]+\", \"\", line)\n",
        "            if len(line) > 5:\n",
        "                questions.append(line)\n",
        "    questions = questions[:5]\n",
        "    state = dict(state)\n",
        "    state[\"questions\"] = questions\n",
        "    print(f\"Generated {len(questions)} questions for {cp.id}\")\n",
        "    return state\n",
        "\n",
        "\n",
        "\n",
        "# Milestone 2: Understanding verification (scoring)\n",
        "# - Strict: counts too-short answers as 0, uses retrieval per-question for focused grading\n",
        "\n",
        "@traceable(name=\"verify_understanding_node\")\n",
        "def verify_understanding(state):\n",
        "\n",
        "    cp = state[\"checkpoint\"]\n",
        "    questions = state.get(\"questions\") or []\n",
        "    answers = state.get(\"learner_answers\") or []\n",
        "    store = state.get(\"temp_vector_store\") or {\"chunks\": state.get(\"processed_chunks\", []), \"vectors\": None, \"meta\": {\"embeddings_used\": False}}\n",
        "\n",
        "    if not questions or not answers:\n",
        "        print(f\"No questions or learner answers for {cp.id}; skipping verification.\")\n",
        "        state = dict(state)\n",
        "        state[\"score_percent\"] = None\n",
        "        state[\"pass_threshold_met\"] = None\n",
        "        return state\n",
        "\n",
        "    n = min(len(questions), len(answers))\n",
        "    if n == 0:\n",
        "        print(f\"No overlapping Q/A pairs for {cp.id}; skipping verification.\")\n",
        "        state = dict(state)\n",
        "        state[\"score_percent\"] = None\n",
        "        state[\"pass_threshold_met\"] = None\n",
        "        return state\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    def meaningful_word_count(text: str) -> int:\n",
        "        if not text or not text.strip():\n",
        "            return 0\n",
        "        words = re.findall(r\"\\w+\", text)\n",
        "        return sum(1 for w in words if len(w) > 2)\n",
        "\n",
        "    for i in range(n):\n",
        "        q = questions[i]\n",
        "        a = answers[i] or \"\"\n",
        "\n",
        "        # quick strict rule: if answer has less than 3 meaningful words → score 0\n",
        "        if meaningful_word_count(a) < 3:\n",
        "            score_val = 0\n",
        "            print(f\"[Verify] {cp.id} Q{i+1} detected as empty/too-short → score: 0\")\n",
        "            scores.append(score_val)\n",
        "            continue\n",
        "\n",
        "        # Retrieve top-k chunks relevant to this question for precise grading\n",
        "        top_chunks = retrieve_top_k(store, query=q, k=3)\n",
        "        context_for_grading = \"\\n\\n\".join(top_chunks)[:4000] if top_chunks else (state.get(\"gathered_context\")[:4000] or \"\")\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are an AI tutor grading a learner's short answer. RESPOND ONLY WITH A SINGLE JSON OBJECT and NOTHING ELSE.\n",
        "\n",
        "CONTEXT (ground truth reference):\n",
        "\\\"\\\"\\\"{context_for_grading}\\\"\\\"\\\"\\n\n",
        "\n",
        "LEARNING OBJECTIVES:\n",
        "{json.dumps(cp.objectives, ensure_ascii=False)}\n",
        "\n",
        "QUESTION:\n",
        "{q}\n",
        "\n",
        "LEARNER ANSWER:\n",
        "{a}\n",
        "\n",
        "SCORING RULES (STRICT):\n",
        "1. If the learner answer is empty, contains only punctuation, or has fewer than 3 meaningful words (words with length >2), return score 0.\n",
        "2. Check whether the answer mentions or correctly paraphrases key concepts from CONTEXT and OBJECTIVES.\n",
        "   - If the answer shows no relevant concepts from the context → return a score in [0,30].\n",
        "   - If the answer shows partial understanding (mentions some correct concepts but misses important parts) → return a score in (30,70).\n",
        "   - If the answer is largely correct and aligned with context → return a score in [70,100].\n",
        "3. Be strict: do not give partial credit for blank or off-topic answers.\n",
        "4. Output MUST be valid JSON only, with keys:\n",
        "   {{ \"score\": <integer 0-100>, \"explanation\": \"<short reason, <=30 words>\" }}\n",
        "\n",
        "Now produce the JSON for this question strictly following rules above.\n",
        "\"\"\"\n",
        "        raw = groq_chat(prompt).strip()\n",
        "\n",
        "        score_val = 0\n",
        "        try:\n",
        "            m = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "            if m:\n",
        "                data = json.loads(m.group(0))\n",
        "                score_val = int(data.get(\"score\", 0))\n",
        "        except Exception:\n",
        "            m2 = re.search(r\"(\\d{1,3})\", raw)\n",
        "            if m2:\n",
        "                score_val = int(m2.group(1))\n",
        "        score_val = max(0, min(100, score_val))\n",
        "        scores.append(score_val)\n",
        "        print(f\"[Verify] {cp.id} Q{i+1} score: {score_val}\")\n",
        "\n",
        "    avg_score = sum(scores) / len(scores) if scores else 0.0\n",
        "    passed = avg_score >= 70.0\n",
        "    state = dict(state)\n",
        "    state[\"score_percent\"] = avg_score\n",
        "    state[\"pass_threshold_met\"] = passed\n",
        "    print(f\"Overall quiz score for {cp.id}: {avg_score:.1f}% (pass >= 70%)\")\n",
        "    return state\n",
        "\n",
        "\n",
        "def feynman_node(state: AgentState) -> AgentState:\n",
        "    cp = state[\"checkpoint\"]\n",
        "    print(\n",
        "        f\"[Feynman Placeholder] Learner did NOT meet 70% threshold for {cp.id} ({cp.topic}). \"\n",
        "        \"In future, this node will trigger Feynman-style explanation & re-teaching.\"\n",
        "    )\n",
        "    return state\n",
        "\n",
        "\n",
        "def route_after_verification(state: AgentState) -> str:\n",
        "    score = state.get(\"score_percent\") or 0.0\n",
        "    if score >= 70.0:\n",
        "        return \"pass\"\n",
        "    else:\n",
        "        return \"feynman\"\n",
        "\n",
        "\n",
        "\n",
        "# Build graph\n",
        "\n",
        "def build_graph():\n",
        "    g = StateGraph(AgentState)\n",
        "    g.add_node(\"start_checkpoint\", start_checkpoint)\n",
        "    g.add_node(\"gather_context\", gather_context)\n",
        "    g.add_node(\"validate_context\", validate_context)\n",
        "    g.add_node(\"process_context\", process_context)\n",
        "    g.add_node(\"generate_questions\", generate_questions)\n",
        "    g.add_node(\"verify_understanding\", verify_understanding)\n",
        "    g.add_node(\"feynman_node\", feynman_node)\n",
        "    g.set_entry_point(\"start_checkpoint\")\n",
        "    g.add_edge(\"start_checkpoint\", \"gather_context\")\n",
        "    g.add_edge(\"gather_context\", \"validate_context\")\n",
        "    g.add_edge(\"validate_context\", \"process_context\")\n",
        "    g.add_edge(\"process_context\", \"generate_questions\")\n",
        "    g.add_edge(\"generate_questions\", \"verify_understanding\")\n",
        "    g.add_conditional_edges(\n",
        "        \"verify_understanding\",\n",
        "        route_after_verification,\n",
        "        {\n",
        "            \"pass\": END,\n",
        "            \"feynman\": \"feynman_node\",\n",
        "        },\n",
        "    )\n",
        "    return g.compile()\n",
        "\n",
        "\n",
        "graph = build_graph()\n",
        "\n",
        "\n",
        "\n",
        "# Helper: read multi-line input (end with an 'END' line)\n",
        "\n",
        "def read_multiline(prompt_msg: str) -> str:\n",
        "    print(prompt_msg)\n",
        "    print(\"Enter/Paste your text. End with a single line containing only: END\")\n",
        "    lines = []\n",
        "    while True:\n",
        "        try:\n",
        "            line = input()\n",
        "        except EOFError:\n",
        "            break\n",
        "        if line.strip() == \"END\":\n",
        "            break\n",
        "        lines.append(line)\n",
        "    return \"\\n\".join(lines).strip()\n",
        "\n",
        "\n",
        "\n",
        "# Interactive per-checkpoint runner\n",
        "\n",
        "def run_single_checkpoint_interactive(cp_id: str) -> dict:\n",
        "    cp = get_checkpoint_by_id(cp_id)\n",
        "    print(f\"\\n--- Checkpoint {cp.id}: {cp.topic} ---\")\n",
        "    notes = read_multiline(\"Provide user notes for this checkpoint (or leave blank and type END):\")\n",
        "    pdfs_input = input(\"Enter comma-separated PDF paths for this checkpoint (or leave blank): \").strip()\n",
        "    pdfs = [p.strip() for p in pdfs_input.split(\",\") if p.strip()] if pdfs_input else []\n",
        "\n",
        "    # initialize state and run nodes up to question generation\n",
        "    state: AgentState = {\n",
        "        \"cp_id\": cp_id,\n",
        "        \"checkpoint\": None,\n",
        "        \"user_notes\": notes,\n",
        "        \"user_pdfs\": pdfs,\n",
        "        \"gathered_context\": \"\",\n",
        "        \"context_sources\": [],\n",
        "        \"relevance_score_model\": None,\n",
        "        \"refetch_attempted\": False,\n",
        "        \"score_meta\": None,\n",
        "        \"processed_chunks\": [],\n",
        "        \"questions\": [],\n",
        "        \"learner_answers\": [],\n",
        "        \"score_percent\": None,\n",
        "        \"pass_threshold_met\": None,\n",
        "        \"temp_vector_store\": None,\n",
        "    }\n",
        "\n",
        "    # start -> gather -> validate -> process -> generate\n",
        "    state = start_checkpoint(state)\n",
        "    state = gather_context(state)\n",
        "    state = validate_context(state)\n",
        "    state = process_context(state)\n",
        "    state = generate_questions(state)\n",
        "\n",
        "    # Display generated questions and collect answers interactively\n",
        "    questions = state.get(\"questions\") or []\n",
        "    print(\"\\nGenerated questions:\")\n",
        "    for i, q in enumerate(questions, start=1):\n",
        "        print(f\"{i}. {q}\")\n",
        "\n",
        "    print(\"\\nNow provide your answers. For each question, paste the answer and end with END.\")\n",
        "    answers = []\n",
        "    for i, q in enumerate(questions, start=1):\n",
        "        ans = read_multiline(f\"\\nAnswer for Q{i}: {q}\")\n",
        "        answers.append(ans)\n",
        "\n",
        "    state[\"learner_answers\"] = answers\n",
        "\n",
        "    # verify\n",
        "    state = verify_understanding(state)\n",
        "\n",
        "    # if failed, run feynman placeholder\n",
        "    if not state.get(\"pass_threshold_met\"):\n",
        "        state = feynman_node(state)\n",
        "\n",
        "    # return summary info\n",
        "    return {\n",
        "        \"cp_id\": cp_id,\n",
        "        \"topic\": cp.topic,\n",
        "        \"context_score\": state.get(\"relevance_score_model\"),\n",
        "        \"quiz_score\": state.get(\"score_percent\"),\n",
        "        \"passed\": state.get(\"pass_threshold_met\"),\n",
        "        \"sources\": state.get(\"context_sources\"),\n",
        "        \"questions\": state.get(\"questions\"),\n",
        "        \"answers\": state.get(\"learner_answers\"),\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation suite (automated tests for Q relevance & scoring)\n",
        "\n",
        "def _generate_good_answer_for_question(context: str, question: str) -> str:\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Use the CONTEXT below to write a concise, correct, and focused answer to the QUESTION.\n",
        "Answer must be at least 25 words (to avoid short-answer penalties) and at most 80 words.\n",
        "Keep it factual and directly relevant to the question.\n",
        "\n",
        "CONTEXT:\n",
        "\\\"\\\"\\\"{context[:3500]}\\\"\\\"\\\"\\n\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    raw = groq_chat(prompt)\n",
        "    txt = raw.strip()\n",
        "    words = re.findall(r\"\\w+\", txt)\n",
        "    if len([w for w in words if len(w) > 2]) < 15:\n",
        "        fallback = f\"Provide a direct, explanatory answer (>=25 words) to: {question}\\nUsing: {context[:800]}\"\n",
        "        raw2 = groq_chat(fallback)\n",
        "        txt = raw2.strip() or txt\n",
        "    return txt\n",
        "\n",
        "\n",
        "def run_evaluation_suite():\n",
        "    \"\"\"\n",
        "    Runs automated evaluation across all CHECKPOINTS.\n",
        "    For each checkpoint:\n",
        "      - gather/process/generate questions\n",
        "      - create 'good' answers via LLM (ensured to be long enough)\n",
        "      - create 'bad' answers (short/off-topic)\n",
        "      - run verify_understanding for both sets and record metrics\n",
        "    \"\"\"\n",
        "    overall = []\n",
        "    print(\"Running automated evaluation suite for all checkpoints...\\n\")\n",
        "    for cp in CHECKPOINTS:\n",
        "        # initialize state\n",
        "        state: AgentState = {\n",
        "            \"cp_id\": cp.id,\n",
        "            \"checkpoint\": None,\n",
        "            \"user_notes\": \"\",  # no notes; will use web search fallback\n",
        "            \"user_pdfs\": [],\n",
        "            \"gathered_context\": \"\",\n",
        "            \"context_sources\": [],\n",
        "            \"relevance_score_model\": None,\n",
        "            \"refetch_attempted\": False,\n",
        "            \"score_meta\": None,\n",
        "            \"processed_chunks\": [],\n",
        "            \"questions\": [],\n",
        "            \"learner_answers\": [],\n",
        "            \"score_percent\": None,\n",
        "            \"pass_threshold_met\": None,\n",
        "            \"temp_vector_store\": None,\n",
        "        }\n",
        "\n",
        "        state = start_checkpoint(state)\n",
        "        state = gather_context(state)\n",
        "        state = validate_context(state)\n",
        "        state = process_context(state)\n",
        "        state = generate_questions(state)\n",
        "\n",
        "        questions = state.get(\"questions\") or []\n",
        "        context_text = state.get(\"gathered_context\") or \"\\n\".join(state.get(\"processed_chunks\", [])) or \"\"\n",
        "        store = state.get(\"temp_vector_store\") or {\"chunks\": [], \"vectors\": None, \"meta\": {\"embeddings_used\": False}}\n",
        "\n",
        "        # Double-confirm embedding usage for this checkpoint\n",
        "        emb_used = bool(store.get(\"vectors\") is not None)\n",
        "        print(f\"[Eval] Checkpoint {cp.id} embeddings_used = {emb_used}\")\n",
        "        if emb_used:\n",
        "            try:\n",
        "                embedding_debug_print(store, label=f\"eval-cp={cp.id}\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Build 'good' answers using the LLM (ensured to be long enough)\n",
        "        good_answers = []\n",
        "        for q in questions:\n",
        "            ans = _generate_good_answer_for_question(context_text, q)\n",
        "            if len(re.findall(r\"\\w+\", ans)) < 30:\n",
        "                ans = ans + \" \" + (\"This answer expands on the main points to ensure full coverage of the objective. \" * 2)\n",
        "            good_answers.append(ans)\n",
        "\n",
        "        # Build 'bad' answers (short/off-topic)\n",
        "        bad_answers = [\"I don't know.\" for _ in questions]\n",
        "\n",
        "        # Evaluate good answers\n",
        "        state_good = dict(state)\n",
        "        state_good[\"learner_answers\"] = good_answers\n",
        "        state_good = verify_understanding(state_good)\n",
        "        good_score = state_good.get(\"score_percent\") or 0.0\n",
        "        good_pass = state_good.get(\"pass_threshold_met\") or False\n",
        "\n",
        "        # Evaluate bad answers\n",
        "        state_bad = dict(state)\n",
        "        state_bad[\"learner_answers\"] = bad_answers\n",
        "        state_bad = verify_understanding(state_bad)\n",
        "        bad_score = state_bad.get(\"score_percent\") or 0.0\n",
        "        bad_pass = state_bad.get(\"pass_threshold_met\") or False\n",
        "\n",
        "        q_rel = 1.0 if questions else 0.0\n",
        "\n",
        "        overall.append({\n",
        "            \"cp_id\": cp.id,\n",
        "            \"topic\": cp.topic,\n",
        "            \"num_questions\": len(questions),\n",
        "            \"q_rel\": q_rel,\n",
        "            \"context_score\": state.get(\"relevance_score_model\"),\n",
        "            \"good_score\": good_score,\n",
        "            \"good_pass\": good_pass,\n",
        "            \"bad_score\": bad_score,\n",
        "            \"bad_pass\": bad_pass,\n",
        "            \"embeddings_used\": emb_used,\n",
        "        })\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n=== Evaluation Summary ===\\n\")\n",
        "    total_q_rel = 0.0\n",
        "    good_pass_count = 0\n",
        "    bad_fail_count = 0\n",
        "    emb_used_count = 0\n",
        "    for r in overall:\n",
        "        print(f\"Checkpoint: {r['cp_id']} - {r['topic']}\")\n",
        "        print(f\"  Questions: {r['num_questions']}, Q-rel: {r['q_rel']:.2f}\")\n",
        "        print(f\"  Context score (1-5): {r['context_score']}\")\n",
        "        print(f\"  Good answers -> score: {r['good_score']:.1f}%, pass: {r['good_pass']}\")\n",
        "        print(f\"  Bad answers  -> score: {r['bad_score']:.1f}%, pass: {r['bad_pass']}\")\n",
        "        print(f\"  Embeddings used: {r.get('embeddings_used')}\")\n",
        "        print()\n",
        "        total_q_rel += r['q_rel']\n",
        "        if r['good_pass']:\n",
        "            good_pass_count += 1\n",
        "        if not r['bad_pass']:\n",
        "            bad_fail_count += 1\n",
        "        if r.get('embeddings_used'):\n",
        "            emb_used_count += 1\n",
        "\n",
        "    n = len(overall) or 1\n",
        "    print(\"--- Overall Metrics ---\")\n",
        "    print(f\"Average question relevance (fraction): {total_q_rel / n:.3f}\")\n",
        "    print(f\"Good answers pass-rate (should be high): {good_pass_count / n * 100:.1f}%\")\n",
        "    print(f\"Bad answers fail-rate (should be high): {bad_fail_count / n * 100:.1f}%\")\n",
        "    print(f\"Embeddings used in checkpoints: {emb_used_count}/{n}\\n\")\n",
        "\n",
        "    return overall\n",
        "\n",
        "\n",
        "\n",
        "# Interactive run for multiple checkpoints\n",
        "\n",
        "def interactive_run():\n",
        "    print(\"Interactive Milestone 2 runner.\")\n",
        "    print(\"Available checkpoints:\")\n",
        "    for cp in CHECKPOINTS:\n",
        "        print(f\" - {cp.id}: {cp.topic}\")\n",
        "\n",
        "    chosen = input(\"Enter comma-separated checkpoint ids to run (or 'all'): \").strip()\n",
        "    if chosen.lower() == \"all\" or not chosen:\n",
        "        ids = [cp.id for cp in CHECKPOINTS]\n",
        "    else:\n",
        "        ids = [c.strip() for c in chosen.split(\",\") if c.strip()]\n",
        "\n",
        "    results = []\n",
        "    for cp_id in ids:\n",
        "        try:\n",
        "            res = run_single_checkpoint_interactive(cp_id)\n",
        "            results.append(res)\n",
        "        except Exception as e:\n",
        "            print(f\"Error running {cp_id}: {e}\")\n",
        "\n",
        "    # Build markdown summary table\n",
        "    lines = []\n",
        "    lines.append(\"### Summary Table\\n\")\n",
        "    lines.append(\"| Topic | Objectives | Sources | Context Score (1–5) | Quiz Score (%) | Pass (>=70%) |\")\n",
        "    lines.append(\"|-------|------------|---------|---------------------|----------------|--------------|\")\n",
        "    for r in results:\n",
        "        cp = get_checkpoint_by_id(r[\"cp_id\"])\n",
        "        obj_list = [f\"- {o}\" for o in cp.objectives]\n",
        "        objectives_str = \"<br>\".join(obj_list)\n",
        "        sources_str = \", \".join(r[\"sources\"]) if r[\"sources\"] else \"None\"\n",
        "        context_score = r[\"context_score\"]\n",
        "        quiz_score = r[\"quiz_score\"]\n",
        "        quiz_display = \"-\" if quiz_score is None else f\"{quiz_score:.1f}\"\n",
        "        passed = r[\"passed\"]\n",
        "        passed_str = \"✅\" if passed else (\"❌\" if passed is not None else \"-\")\n",
        "        lines.append(f\"| {r['topic']} | {objectives_str} | {sources_str} | {context_score} | {quiz_display} | {passed_str} |\")\n",
        "\n",
        "    table_md = \"\\n\".join(lines)\n",
        "    try:\n",
        "        display(Markdown(table_md))\n",
        "    except Exception:\n",
        "        print(table_md)\n",
        "\n",
        "    # Save ALL raw LLM prompt+response logs into ONE PDF\n",
        "    save_all_raw_to_one_pdf(\"all_llm_raw_output.pdf\")\n",
        "\n",
        "\n",
        "# Main\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Milestone 1 & 2 runner\")\n",
        "    print(\"Options:\\n  1) interactive run (generate questions & answer interactively)\\n  2) run evaluation suite (automated tests for Q relevance & scoring)\")\n",
        "    choice = input(\"Enter 1 or 2 (default 1): \").strip() or \"1\"\n",
        "    if choice == \"2\":\n",
        "        run_evaluation_suite()\n",
        "    else:\n",
        "        interactive_run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hhyPvU4CdrN",
        "outputId": "c2e966e4-f77c-4ac4-be01-77620caad824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Embedding] Loading SentenceTransformer 'all-MiniLM-L6-v2' ... (this may take a few seconds)\n",
            "[Embedding] Model loaded:all-MiniLM-L6-v2\n",
            "Milestone 1 & 2 runner\n",
            "Options:\n",
            "  1) interactive run (generate questions & answer interactively)\n",
            "  2) run evaluation suite (automated tests for Q relevance & scoring)\n",
            "Enter 1 or 2 (default 1): 2\n",
            "Running automated evaluation suite for all checkpoints...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "[Gathered Context] Source(s): web_search\n",
            "--------------------------------------------------------------------------------\n",
            "**Forward Propagation in Neural Networks**\n",
            "\n",
            "**Objective:** Calculate the activations at each neuron for each successive hidden layer until arriving at the output layer.\n",
            "\n",
            "**Key Components:**\n",
            "\n",
            "1. **Neural Network Layers:** Input, Hidden, and Output layers.\n",
            "2. **Neurons:** Each layer consists of several neurons that receive and process input data.\n",
            "3. **Forward Propagation:** The process of passing input data through the network, generating an output (prediction) at each layer.\n",
            "\n",
            "**Step-by-Step Process:**\n",
            "\n",
            "1. Input data is passed through the input layer.\n",
            "2. Each neuron in the hidden layer receives the output of each neuron in the previous layer.\n",
            "3. The output of each neuron is calculated using an activation function.\n",
            "4. The output of each hidden layer is passed to the next layer until reaching the output layer.\n",
            "5. The final output is generated at the output layer.\n",
            "\n",
            "**Importance:** Understanding forward propagation is crucial for building and training neural networks, as it lays the foundation for backpropagation, which is used to optimize the network's performance.\n",
            "================================================================================\n",
            "\n",
            "Score for cp1: 5 (3/3 objectives covered)\n",
            " - Obj: Understand what an artificial neuron is... => yes | evidence: Each layer consists of several neurons that receive and process input data.\n",
            " - Obj: Understand input, hidden, and output layers... => yes | evidence: The process of passing input data through the network, generating an output (prediction) at each layer, is called forward propagation.\n",
            " - Obj: Understand the concept of forward propagation... => yes | evidence: Forward Propagation is the process of passing input data through the network, generating an output (prediction) at each layer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1704944128.py:448: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  similarity = float(ctx_vec @ ref_vec.T)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 3 questions for cp1\n",
            "[Eval] Checkpoint cp1 embeddings_used = True\n",
            "[Embedding Confirm] eval-cp=cp1 embeddings used: True | vector shape: (1, 384)\n",
            "[Embedding Confirm] sample vec[0][:6] ~ [-0.0344725176692009, -0.0859455019235611, 0.007470434997230768, -0.011288967914879322, 0.0015978480223566294, 0.035555195063352585]\n",
            "[Verify] cp1 Q1 score: 90\n",
            "[Verify] cp1 Q2 score: 90\n",
            "[Verify] cp1 Q3 score: 90\n",
            "Overall quiz score for cp1: 90.0% (pass >= 70%)\n",
            "[Verify] cp1 Q1 detected as empty/too-short → score: 0\n",
            "[Verify] cp1 Q2 detected as empty/too-short → score: 0\n",
            "[Verify] cp1 Q3 detected as empty/too-short → score: 0\n",
            "Overall quiz score for cp1: 0.0% (pass >= 70%)\n",
            "\n",
            "================================================================================\n",
            "[Gathered Context] Source(s): web_search\n",
            "--------------------------------------------------------------------------------\n",
            "**Gradient Descent: An Optimization Algorithm for Machine Learning**\n",
            "\n",
            "**Key Points:**\n",
            "\n",
            "1. **Definition:** Gradient descent is an iterative optimization algorithm used to find the best weights and bias for a linear regression model by minimizing the loss function.\n",
            "2. **Convergence:** A model is considered to have converged when further iterations do not significantly reduce the loss, indicating it has found the weights and bias that produce the lowest possible loss.\n",
            "3. **Process:** Gradient descent iteratively finds the weights and bias that produce the model with the lowest loss by adjusting parameters in the direction of the negative gradient.\n",
            "4. **Goal:** The goal of gradient descent is to minimize the cost function, or the error between predicted and actual y.\n",
            "5. **Types:** Batch gradient descent sums the error for each point in a training set, updating the model only after all training examples have been evaluated.\n",
            "\n",
            "**Key Applications:**\n",
            "\n",
            "1. **Machine Learning:** Gradient descent is commonly used to train machine learning models and neural networks.\n",
            "2. **Artificial Intelligence (AI):** Optimized machine learning models can be powerful tools for AI and computer science applications.\n",
            "================================================================================\n",
            "\n",
            "Score for cp2: 5 (3/3 objectives covered)\n",
            " - Obj: Understand loss minimization... => yes | evidence: The goal of gradient descent is to minimize the cost function, or the error between predicted and actual y.\n",
            " - Obj: Understand gradient as slope of loss... => yes | evidence: Gradient descent iteratively finds the weights and bias that produce the model with the lowest loss by adjusting parameters in the direction\n",
            " - Obj: Understand iterative parameter updates... => yes | evidence: Gradient descent iteratively finds the weights and bias that produce the model with the lowest loss by adjusting parameters in the direction\n",
            "Generated 3 questions for cp2\n",
            "[Eval] Checkpoint cp2 embeddings_used = True\n",
            "[Embedding Confirm] eval-cp=cp2 embeddings used: True | vector shape: (1, 384)\n",
            "[Embedding Confirm] sample vec[0][:6] ~ [-0.09509024024009705, -0.02141891047358513, -0.060603879392147064, 0.009882657788693905, -0.00042758428025990725, 0.02721959725022316]\n",
            "[Verify] cp2 Q1 score: 90\n",
            "[Verify] cp2 Q2 score: 90\n",
            "[Verify] cp2 Q3 score: 90\n",
            "Overall quiz score for cp2: 90.0% (pass >= 70%)\n",
            "[Verify] cp2 Q1 detected as empty/too-short → score: 0\n",
            "[Verify] cp2 Q2 detected as empty/too-short → score: 0\n",
            "[Verify] cp2 Q3 detected as empty/too-short → score: 0\n",
            "Overall quiz score for cp2: 0.0% (pass >= 70%)\n",
            "\n",
            "================================================================================\n",
            "[Gathered Context] Source(s): web_search\n",
            "--------------------------------------------------------------------------------\n",
            "**Activation Functions in Neural Networks:**\n",
            "\n",
            "Activation functions introduce non-linearity to neural networks, enabling them to learn complex patterns. There are three primary activation functions:\n",
            "\n",
            "1. **Sigmoid**: Maps input values to a range between 0 and 1, commonly used in binary classification.\n",
            "2. **Tanh (Hyperbolic Tangent)**: Maps input values to a range between -1 and 1, useful in hidden layers for zero-centered data.\n",
            "3. **Rectified Linear Unit (ReLU)**: A piecewise linear function that outputs the input value directly if it's positive, otherwise returns zero. ReLU is less susceptible to the vanishing gradient problem and often works better than smooth functions like sigmoid or tanh.\n",
            "\n",
            "**Choosing the Right Activation Function:**\n",
            "\n",
            "* Use sigmoid for binary classification.\n",
            "* Use tanh for hidden layers with zero-centered data.\n",
            "* Use ReLU for deep neural networks, especially in CNNs and large architectures.\n",
            "\n",
            "**Key Benefits of Activation Functions:**\n",
            "\n",
            "* Introduce non-linearity to neural networks.\n",
            "* Enable learning of complex patterns and relationships.\n",
            "* Help assign different levels of importance to different inputs.\n",
            "* Model complex relationships between input and output.\n",
            "================================================================================\n",
            "\n",
            "Score for cp3: 5 (2/2 objectives covered)\n",
            " - Obj: Know common activations (sigmoid, tanh, relu)... => yes | evidence: There are three primary activation functions: sigmoid, tanh, and ReLU.\n",
            " - Obj: Understand when/why to use each... => yes | evidence: Use sigmoid for binary classification, tanh for hidden layers with zero-centered data, and ReLU for deep neural networks, especially in CNNs\n",
            "Generated 4 questions for cp3\n",
            "[Eval] Checkpoint cp3 embeddings_used = True\n",
            "[Embedding Confirm] eval-cp=cp3 embeddings used: True | vector shape: (1, 384)\n",
            "[Embedding Confirm] sample vec[0][:6] ~ [-0.0762467086315155, -0.08232060074806213, 0.02537688985466957, -0.021133117377758026, -0.055034056305885315, 0.025631951168179512]\n",
            "[Verify] cp3 Q1 score: 80\n",
            "[Verify] cp3 Q2 score: 90\n",
            "[Verify] cp3 Q3 score: 80\n",
            "[Verify] cp3 Q4 score: 90\n",
            "Overall quiz score for cp3: 85.0% (pass >= 70%)\n",
            "[Verify] cp3 Q1 detected as empty/too-short → score: 0\n",
            "[Verify] cp3 Q2 detected as empty/too-short → score: 0\n",
            "[Verify] cp3 Q3 detected as empty/too-short → score: 0\n",
            "[Verify] cp3 Q4 detected as empty/too-short → score: 0\n",
            "Overall quiz score for cp3: 0.0% (pass >= 70%)\n",
            "\n",
            "================================================================================\n",
            "[Gathered Context] Source(s): web_search\n",
            "--------------------------------------------------------------------------------\n",
            "**Backpropagation: Efficient Gradient Computation**\n",
            "\n",
            "Backpropagation is a method used to compute the gradient of a loss function with respect to the weights of a neural network. It efficiently calculates gradients by propagating errors backward through the network, starting from the last layer and iterating backward.\n",
            "\n",
            "**Key Steps:**\n",
            "\n",
            "1. **Forward Pass**: The network processes input data and produces an output.\n",
            "2. **Backward Pass**: Errors are propagated backward through the network, using the chain rule of calculus to compute gradients.\n",
            "3. **Weight Update**: Weights are updated using gradient descent, specifically the Widrow-Hoff delta rule.\n",
            "\n",
            "**Key Concepts:**\n",
            "\n",
            "* Chain rule of calculus\n",
            "* Gradient descent\n",
            "* Weight update using the Widrow-Hoff delta rule\n",
            "\n",
            "**Example:**\n",
            "\n",
            "Backpropagation can be illustrated with a simple example, where errors are propagated backward through a network to compute gradients on inputs.\n",
            "================================================================================\n",
            "\n",
            "Score for cp4: 5 (2/2 objectives covered)\n",
            " - Obj: Understand chain rule for gradients... => yes | evidence: Backpropagation efficiently calculates gradients by propagating errors backward through the network, using the chain rule of calculus to com\n",
            " - Obj: Understand how weight updates propagate backward... => yes | evidence: Backpropagation efficiently calculates gradients by propagating errors backward through the network, starting from the last layer and iterat\n",
            "Generated 3 questions for cp4\n",
            "[Eval] Checkpoint cp4 embeddings_used = True\n",
            "[Embedding Confirm] eval-cp=cp4 embeddings used: True | vector shape: (1, 384)\n",
            "[Embedding Confirm] sample vec[0][:6] ~ [-0.0794261023402214, -0.031722668558359146, -0.01417860109359026, 0.005147431045770645, -0.02887505479156971, -0.01938037946820259]\n",
            "[Verify] cp4 Q1 score: 90\n",
            "[Verify] cp4 Q2 score: 90\n",
            "[Verify] cp4 Q3 score: 90\n",
            "Overall quiz score for cp4: 90.0% (pass >= 70%)\n",
            "[Verify] cp4 Q1 detected as empty/too-short → score: 0\n",
            "[Verify] cp4 Q2 detected as empty/too-short → score: 0\n",
            "[Verify] cp4 Q3 detected as empty/too-short → score: 0\n",
            "Overall quiz score for cp4: 0.0% (pass >= 70%)\n",
            "\n",
            "=== Evaluation Summary ===\n",
            "\n",
            "Checkpoint: cp1 - Basics of Neural Networks\n",
            "  Questions: 3, Q-rel: 1.00\n",
            "  Context score (1-5): 5\n",
            "  Good answers -> score: 90.0%, pass: True\n",
            "  Bad answers  -> score: 0.0%, pass: False\n",
            "  Embeddings used: True\n",
            "\n",
            "Checkpoint: cp2 - Gradient Descent\n",
            "  Questions: 3, Q-rel: 1.00\n",
            "  Context score (1-5): 5\n",
            "  Good answers -> score: 90.0%, pass: True\n",
            "  Bad answers  -> score: 0.0%, pass: False\n",
            "  Embeddings used: True\n",
            "\n",
            "Checkpoint: cp3 - Activation Functions\n",
            "  Questions: 4, Q-rel: 1.00\n",
            "  Context score (1-5): 5\n",
            "  Good answers -> score: 85.0%, pass: True\n",
            "  Bad answers  -> score: 0.0%, pass: False\n",
            "  Embeddings used: True\n",
            "\n",
            "Checkpoint: cp4 - Backpropagation\n",
            "  Questions: 3, Q-rel: 1.00\n",
            "  Context score (1-5): 5\n",
            "  Good answers -> score: 90.0%, pass: True\n",
            "  Bad answers  -> score: 0.0%, pass: False\n",
            "  Embeddings used: True\n",
            "\n",
            "--- Overall Metrics ---\n",
            "Average question relevance (fraction): 1.000\n",
            "Good answers pass-rate (should be high): 100.0%\n",
            "Bad answers fail-rate (should be high): 100.0%\n",
            "Embeddings used in checkpoints: 4/4\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0AxOJVjwF7Tp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}