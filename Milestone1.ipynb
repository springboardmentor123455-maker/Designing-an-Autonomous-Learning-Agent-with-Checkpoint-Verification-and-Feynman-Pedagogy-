{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9970f0e5-0180-4fbd-b6b9-88808783763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langgraph langchain langchain-huggingface langchain-community duckduckgo-search python-dotenv pypdf faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a01cfa1-cc36-47ea-a79c-3bf75a61d0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Hugging Face API Token:  ········\n",
      "Enter LangSmith API Key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Configured Successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def get_mandatory_env(var_name, prompt):\n",
    "    val = os.environ.get(var_name)\n",
    "    if not val:\n",
    "        val = getpass.getpass(prompt)\n",
    "        os.environ[var_name] = val\n",
    "    if not val:\n",
    "        raise ValueError(f\"{var_name} is MANDATORY. Please provide it.\")\n",
    "    return val\n",
    "\n",
    "# 1. Hugging Face Token \n",
    "get_mandatory_env(\"HUGGINGFACEHUB_API_TOKEN\", \"Enter Hugging Face API Token: \")\n",
    "\n",
    "# 2. LangSmith Configuration \n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Llama-Tutor-Agent\"\n",
    "get_mandatory_env(\"LANGCHAIN_API_KEY\", \"Enter LangSmith API Key: \")\n",
    "\n",
    "print(\"Environment Configured Successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40352286-26fe-4ca8-be8b-375c7ffda9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Ready: Qwen/Qwen2.5-72B-Instruct\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace, HuggingFaceEmbeddings\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# MODEL CONFIGURATION\n",
    "MODEL_REPO = \"Qwen/Qwen2.5-72B-Instruct\" \n",
    "\n",
    "# 1. Initialize Endpoint\n",
    "llm_engine = HuggingFaceEndpoint(\n",
    "    repo_id=MODEL_REPO,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    streaming=False, \n",
    ")\n",
    "\n",
    "# 2. Wrap in Chat Interface \n",
    "llm = ChatHuggingFace(llm=llm_engine)\n",
    "\n",
    "# 3. Initialize Embeddings & Search\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "web_search = DuckDuckGoSearchRun()\n",
    "\n",
    "# 4. Define Checkpoints\n",
    "CHECKPOINTS = {\n",
    "    1: {\"topic\": \"Transformer Architecture\", \"objective\": \"Explain the Encoder-Decoder structure and Self-Attention mechanism.\"},\n",
    "    2: {\"topic\": \"Backpropagation\", \"objective\": \"Detail the chain rule and how gradients update weights in a neural network.\"},\n",
    "    3: {\"topic\": \"RAG Systems\", \"objective\": \"Explain Retrieval-Augmented Generation, vector databases, and semantic search.\"},\n",
    "    4: {\"topic\": \"Generative Adversarial Networks\", \"objective\": \"Explain the Generator vs Discriminator dynamic and training challenges\"},\n",
    "    5: {\"topic\": \"Convolutional Neural Networks\", \"objective\": \"Explain Filters, Pooling layers, and their application in image recognition.\"}\n",
    "}\n",
    "\n",
    "print(f\"Model Ready: {MODEL_REPO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af54023e-97fd-42a8-9198-4ea1335cb058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT A LEARNING CHECKPOINT\n",
      "1. Transformer Architecture\n",
      "2. Backpropagation\n",
      "3. RAG Systems\n",
      "4. Generative Adversarial Networks\n",
      "5. Convolutional Neural Networks\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-5):  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: RAG Systems\n",
      "Auto-detected file: notes.pdf\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: User Interaction \n",
    "import os\n",
    "\n",
    "DEFAULT_FILE = \"notes.pdf\" \n",
    "\n",
    "# 1. Select Checkpoint\n",
    "print(\"SELECT A LEARNING CHECKPOINT\")\n",
    "if 'CHECKPOINTS' not in globals():\n",
    "    print(\"Error: CHECKPOINTS dict missing. Please Run Cell 3 first.\")\n",
    "else:\n",
    "    for key, val in CHECKPOINTS.items():\n",
    "        print(f\"{key}. {val['topic']}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            choice_input = input(\"Enter choice (1-5): \")\n",
    "            choice = int(choice_input)\n",
    "            if choice in CHECKPOINTS:\n",
    "                selected_checkpoint = CHECKPOINTS[choice]\n",
    "                print(f\"Selected: {selected_checkpoint['topic']}\")\n",
    "                break\n",
    "            print(\"Invalid choice.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a number.\")\n",
    "\n",
    "    # 2. Auto-Detect Document\n",
    "    if os.path.exists(DEFAULT_FILE):\n",
    "        print(f\"Auto-detected file: {DEFAULT_FILE}\")\n",
    "        pdf_path = DEFAULT_FILE\n",
    "    else:\n",
    "        # Fallback if file isn't there\n",
    "        user_input = input(f\"File '{DEFAULT_FILE}' not found. Enter filename manually: \")\n",
    "        pdf_path = user_input if user_input else None\n",
    "\n",
    "    if pdf_path and not os.path.exists(pdf_path):\n",
    "        print(f\"Warning: File '{pdf_path}' not found. Agent will rely on Web Search.\")\n",
    "        pdf_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9866e3d8-bc1c-4d8b-8df5-0eed78a38ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Optional\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    topic: str\n",
    "    objective: str\n",
    "    file_path: Optional[str]\n",
    "    \n",
    "    # Content Storage\n",
    "    doc_context: str\n",
    "    web_context: str\n",
    "    \n",
    "    # Flags & Counters\n",
    "    needs_web_search: bool\n",
    "    retries: int\n",
    "    \n",
    "    # Output & Scoring\n",
    "    final_essay: str\n",
    "    relevance_score: int\n",
    "    validation_reasoning: str\n",
    "    \n",
    "    # Best Effort Memory\n",
    "    best_essay: str\n",
    "    best_score: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee199a09-72c4-444d-8fdd-7b1e9273de95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Document Processing Node \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "def check_user_doc(state: AgentState):\n",
    "    print(\"CHECKING USER DOCUMENT\")\n",
    "    file_path = state[\"file_path\"]\n",
    "    topic = state[\"topic\"]\n",
    "    \n",
    "    # 1. Basic Check\n",
    "    if not file_path:\n",
    "        return {\"doc_context\": \"\", \"needs_web_search\": True}\n",
    "    \n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        splits = splitter.split_documents(docs)\n",
    "        \n",
    "        if not splits:\n",
    "            return {\"doc_context\": \"\", \"needs_web_search\": True}\n",
    "        \n",
    "        # 2. Vector Search\n",
    "        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "        retrieved_docs = retriever.invoke(topic)\n",
    "        context_text = \"\\n\\n\".join([d.page_content for d in retrieved_docs])\n",
    "        \n",
    "        # 3. LLM GRADED CHECK\n",
    "        grader_prompt = f\"\"\"\n",
    "        You are a Relevance Grader. \n",
    "        Topic: {topic}\n",
    "        Retrieved Text: {context_text[:2000]}\n",
    "        \n",
    "        Does the retrieved text contain a DETAILED explanation of the Topic?\n",
    "        If it only contains headers, unrelated topics, or placeholders like \"This chapter is blank\", answer NO.\n",
    "        \n",
    "        Answer only YES or NO.\n",
    "        \"\"\"\n",
    "        response = llm.invoke([HumanMessage(content=grader_prompt)])\n",
    "        grade = response.content.strip().upper()\n",
    "        \n",
    "        if \"NO\" in grade or len(context_text) < 200:\n",
    "            print(f\"Doc content found ({len(context_text)} chars) but graded INCOMPLETE. Enabling Web Search.\")\n",
    "            return {\"doc_context\": context_text, \"needs_web_search\": True}\n",
    "        \n",
    "        print(f\"Found relevant content in user doc. (Grader: {grade})\")\n",
    "        return {\"doc_context\": context_text, \"needs_web_search\": False}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading doc: {e}\")\n",
    "        return {\"doc_context\": \"\", \"needs_web_search\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3764eaa-df44-407a-936d-8a8bc2410d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_web_search(state: AgentState):\n",
    "    print(\"PERFORMING WEB SEARCH\")\n",
    "    \n",
    "    if not state.get(\"needs_web_search\"):\n",
    "        print(\"Skipping web search (User Doc sufficient).\")\n",
    "        return {\"web_context\": \"\"}\n",
    "        \n",
    "    topic = state[\"topic\"]\n",
    "    objective = state[\"objective\"]\n",
    "    \n",
    "    query = f\"{topic} {objective} detailed technical explanation\"\n",
    "    try:\n",
    "        search_results = web_search.invoke(query)\n",
    "        print(\"Web search completed.\")\n",
    "        return {\"web_context\": search_results}\n",
    "    except Exception as e:\n",
    "        print(f\"Search failed: {e}\")\n",
    "        return {\"web_context\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df91fbc9-c920-41c9-93ec-5d98b14e9cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Generation Node\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "def generate_essay(state: AgentState):\n",
    "    print(\"GENERATING SUMMARY\")\n",
    "    \n",
    "    doc_text = state.get(\"doc_context\", \"\")\n",
    "    web_text = state.get(\"web_context\", \"\")\n",
    "    topic = state[\"topic\"]\n",
    "    objective = state[\"objective\"]\n",
    "    \n",
    "    system_instruction =\"\"\"You are an expert AI Tutor. Write a CONCISE Technical Summary (approx 3 paragraphs).\n",
    "    \n",
    "    RULES:\n",
    "    1. Keep it under 400 words.\n",
    "    2. STRICT CITATIONS: End sentences with [Source: User Doc] or [Source: Web Search].\n",
    "    3. If User Doc is irrelevant/blank, rely fully on Web Search.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_content = f\"\"\"\n",
    "    TOPIC: {topic}\n",
    "    OBJECTIVE: {objective}\n",
    "    \n",
    "    CONTEXT FROM USER DOC:\n",
    "    {doc_text[:3000]}\n",
    "    \n",
    "    CONTEXT FROM WEB SEARCH:\n",
    "    {web_text[:3000]}\n",
    "    \n",
    "    Write the summary now.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=system_instruction),\n",
    "        HumanMessage(content=user_content)\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response_msg = llm.invoke(messages)\n",
    "        return {\"final_essay\": response_msg.content}\n",
    "    except Exception as e:\n",
    "        print(f\"\\n GENERATION ERROR: {e}\\n\")\n",
    "        return {\"final_essay\": f\"Error generating essay: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3df67eae-ee8a-4df1-bb34-d7892673136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Validation Node\n",
    "import re\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def validate_output(state: AgentState):\n",
    "    print(\"VALIDATING OUTPUT\")\n",
    "    essay = state[\"final_essay\"]\n",
    "    objective = state[\"objective\"]\n",
    "    current_retries = state.get(\"retries\", 0)\n",
    "    best_score = state.get(\"best_score\", 0)\n",
    "    best_essay = state.get(\"best_essay\", \"\")\n",
    "    \n",
    "    if \"Error generating essay\" in essay:\n",
    "        score = 1\n",
    "        reason = \"Generation Failed\"\n",
    "    else:\n",
    "        # UPDATED CHECKLIST\n",
    "        prompt = f\"\"\"\n",
    "        Grade the following summary on a scale of 1-5 using this EXACT checklist:\n",
    "        \n",
    "        +1 Point: Is it relevant to \"{objective}\"?\n",
    "        +1 Point: Is it clear and concise?\n",
    "        +1 Point: Does it contain citations like [Source: User Doc] or [Source: Web Search]?\n",
    "        +1 Point: Is the content technically accurate?\n",
    "        +1 Point: Is the structure clear?\n",
    "        \n",
    "        Total Score = Sum of points. (Max 5).\n",
    "        \n",
    "        Return ONLY valid JSON: {{\"score\": int, \"reasoning\": \"string\"}}\n",
    "        \n",
    "        SUMMARY START:\n",
    "        {essay[:1500]}...\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = llm.invoke([HumanMessage(content=prompt)])\n",
    "            json_match = re.search(r\"\\{.*\\}\", response.content, re.DOTALL)\n",
    "            if json_match:\n",
    "                data = json.loads(json_match.group(0))\n",
    "                score = data.get(\"score\", 0)\n",
    "                reason = data.get(\"reasoning\", \"No reasoning\")\n",
    "            else:\n",
    "                score = 4 \n",
    "                reason = \"JSON Parse Error, but content generated.\"\n",
    "        except Exception as e:\n",
    "            print(f\"Validation failed: {e}\")\n",
    "            score = 3\n",
    "            reason = \"Validation Exception\"\n",
    "\n",
    "    print(f\"Attempt {current_retries + 1} Score: {score}/5 | Reason: {reason}\")\n",
    "\n",
    "    if score > best_score:\n",
    "        print(f\"New High Score! ({score})\")\n",
    "        best_score = score\n",
    "        best_essay = essay\n",
    "\n",
    "    return {\n",
    "        \"relevance_score\": score, \n",
    "        \"validation_reasoning\": reason,\n",
    "        \"retries\": current_retries + 1,\n",
    "        \"best_score\": best_score,\n",
    "        \"best_essay\": best_essay\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39bd2ae2-6ca6-4eeb-bb07-f9f826e6b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def check_retry(state: AgentState):\n",
    "    score = state[\"relevance_score\"]\n",
    "    retries = state[\"retries\"]\n",
    "    \n",
    "    # Success\n",
    "    if score >= 4:\n",
    "        print(\"Score meets threshold. Finishing.\")\n",
    "        return \"success\"\n",
    "    \n",
    "    # Max Retries\n",
    "    if retries >= 5:\n",
    "        print(\"Max retries reached. Accepting best effort.\")\n",
    "        return \"max_retries\"\n",
    "    \n",
    "    # Retry with Delay\n",
    "    print(\"Score too low. Retrying in 2 seconds...\")\n",
    "    time.sleep(2)\n",
    "    return \"retry\"\n",
    "\n",
    "def finalize_submission(state: AgentState):\n",
    "    print(\"FINALIZING SUBMISSION\")\n",
    "    return {\n",
    "        \"final_essay\": state[\"best_essay\"], \n",
    "        \"relevance_score\": state[\"best_score\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "665d24f8-da12-41b7-ab2f-156a14db4922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust Learning Graph Compiled.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add Nodes\n",
    "workflow.add_node(\"check_user_doc\", check_user_doc)\n",
    "workflow.add_node(\"perform_web_search\", perform_web_search)\n",
    "workflow.add_node(\"generate_essay\", generate_essay)\n",
    "workflow.add_node(\"validate_output\", validate_output)\n",
    "workflow.add_node(\"finalize_submission\", finalize_submission)\n",
    "\n",
    "# Set Entry Point\n",
    "workflow.set_entry_point(\"check_user_doc\")\n",
    "\n",
    "# Standard Edges\n",
    "workflow.add_edge(\"check_user_doc\", \"perform_web_search\")\n",
    "workflow.add_edge(\"perform_web_search\", \"generate_essay\")\n",
    "workflow.add_edge(\"generate_essay\", \"validate_output\")\n",
    "\n",
    "# Conditional Edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"validate_output\",\n",
    "    check_retry,\n",
    "    {\n",
    "        \"success\": \"finalize_submission\",\n",
    "        \"max_retries\": \"finalize_submission\",\n",
    "        \"retry\": \"generate_essay\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"finalize_submission\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "print(\"Robust Learning Graph Compiled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dce8bbeb-95d0-4e3e-9a5f-22b32a5a1df6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Robust Agent for: RAG Systems\n",
      "CHECKING USER DOCUMENT\n",
      "Doc content found (3129 chars) but graded INCOMPLETE. Enabling Web Search.\n",
      "PERFORMING WEB SEARCH\n",
      "Web search completed.\n",
      "GENERATING SUMMARY\n",
      "VALIDATING OUTPUT\n",
      "Attempt 1 Score: 5/5 | Reason: The summary is relevant to the topic, explaining Retrieval-Augmented Generation, vector databases, and semantic search. It is clear and concise, providing examples and explanations that are easy to understand. The content includes citations from both user documentation and web searches. The technical accuracy is maintained throughout the summary, and the structure is clear and logical.\n",
      "New High Score! (5)\n",
      "Score meets threshold. Finishing.\n",
      "FINALIZING SUBMISSION\n",
      "\n",
      "==================================================\n",
      "FINAL OUTPUT (Score: 5/5)\n",
      "==================================================\n",
      "Retrieval-Augmented Generation (RAG) is a technique designed to enhance the output of large language models (LLMs) by integrating an external knowledge base. Unlike traditional LLMs that rely solely on their pre-trained data, RAG systems can access and utilize up-to-date and domain-specific information, leading to more accurate and relevant responses [Source: User Doc]. For example, when an employee queries, \"How much annual leave do I have?\", the RAG system retrieves the company's leave policy and the employee's leave records, ensuring the response is both accurate and contextually relevant [Source: Web Search].\n",
      "\n",
      "Vector databases play a crucial role in RAG systems by storing and retrieving information based on semantic similarity rather than exact keyword matches. These databases convert text into high-dimensional vectors, where each dimension represents a feature of the text. When a query is made, the system computes the vector representation of the query and finds the closest vectors in the database, which correspond to the most relevant documents or pieces of information. This approach ensures that the retrieved data is semantically similar to the query, enhancing the quality of the generated response [Source: Web Search].\n",
      "\n",
      "Semantic search algorithms further refine the retrieval process by understanding the meaning and context of the query. Unlike traditional keyword-based search, semantic search considers the intent behind the query and the relationships between words and concepts. This is achieved through techniques like word embeddings and natural language processing (NLP), which map words and phrases to vector spaces where similar meanings are close to each other. By leveraging these advanced search methods, RAG systems can provide more precise and contextually appropriate information, significantly improving the overall performance and reliability of the model [Source: Web Search].\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Execute Agent\n",
    "inputs = {\n",
    "    \"topic\": selected_checkpoint[\"topic\"],\n",
    "    \"objective\": selected_checkpoint[\"objective\"],\n",
    "    \"file_path\": pdf_path,\n",
    "    \"needs_web_search\": False, \n",
    "    \"doc_context\": \"\",\n",
    "    \"web_context\": \"\",\n",
    "    # Initialize counters\n",
    "    \"retries\": 0,\n",
    "    \"best_score\": 0,\n",
    "    \"best_essay\": \"\"\n",
    "}\n",
    "\n",
    "print(f\"Starting Robust Agent for: {inputs['topic']}\")\n",
    "\n",
    "# Run the agent ONLY ONCE\n",
    "result = app.invoke(inputs)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"FINAL OUTPUT (Score: {result['relevance_score']}/5)\")\n",
    "print(\"=\"*50)\n",
    "print(result[\"final_essay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0813c322-c93c-4073-b076-1a18d092308f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b7648e-9542-48ee-b67d-dbbe19a1a6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
